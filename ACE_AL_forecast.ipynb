{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "57174503",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate (future) ACE from forecasts of a single intensity model using cyclone a-tracks\n",
    "### Uses linear interpolation to get values of 6h intervals\n",
    "\n",
    "# download latest a-tracks manually for basin of interest (no automation yet)\n",
    "# and change names of atrack files below\n",
    "\n",
    "# note: make sure to gunzip them\n",
    "# https://ftp.nhc.noaa.gov/atcf/aid_public/?C=M;O=D\n",
    "    \n",
    "### DOWNLOAD LATEST ATRACKS AND UPDATE THE FOLLOWING TWO VARIABLES EVERY RUN OF THE NOTEBOOK\n",
    "atrack_files = [\n",
    "    'aal152023.dat'\n",
    "]\n",
    "\n",
    "# latest real time ACE (for North Atlantic)\n",
    "# from http://tropical.atmos.colostate.edu/Realtime/\n",
    "real_time_ACE = 110.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "03ca0169",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### CONFIG BASIN AND INTENSITY MODEL\n",
    "match_basin = 'AL'\n",
    "# consensus model\n",
    "match_model = 'IVCN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "04b3f4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_column_names = [\n",
    "    'BASIN',\n",
    "    'CY',\n",
    "    'YYYYMMDDHH',\n",
    "    'TECHNUM/MIN',\n",
    "    'TECH',\n",
    "    'TAU',\n",
    "    'LatN/S',\n",
    "    'LonE/W',\n",
    "    'VMAX',\n",
    "    'MSLP',\n",
    "    'TY',\n",
    "    'RAD',\n",
    "    'WINDCODE',\n",
    "    'RAD1',\n",
    "    'RAD2',\n",
    "    'RAD3',\n",
    "    'RAD4',\n",
    "    'POUTER',\n",
    "    'ROUTER',\n",
    "    'RMW',\n",
    "    'GUSTS',\n",
    "    'EYE',\n",
    "    'SUBREGION',\n",
    "    'MAXSEAS',\n",
    "    'INITIALS',\n",
    "    'DIR',\n",
    "    'SPEED',\n",
    "    'STORMNAME',\n",
    "    'DEPTH',\n",
    "    'SEAS',\n",
    "    'SEASCODE',\n",
    "    'SEAS1',\n",
    "    'SEAS2',\n",
    "    'SEAS3',\n",
    "    'SEAS4',\n",
    "    'USERDEFINED1',\n",
    "    'userdata1',\n",
    "    'USERDEFINED2',\n",
    "    'userdata2',\n",
    "    'USERDEFINED3',\n",
    "    'userdata3',\n",
    "    'USERDEFINED4',\n",
    "    'userdata4',\n",
    "    'USERDEFINED5',\n",
    "    'userdata5'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "0c68ef4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "track_line_lists = []\n",
    "\n",
    "# read the tracks\n",
    "for atrack_file in atrack_files:\n",
    "    with open(atrack_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    for line in lines:\n",
    "        line_list = line.split(',')\n",
    "        line_list_trim = [x.strip() for x in line_list]\n",
    "        line_list_trim_rempty = line_list_trim\n",
    "        \n",
    "        # trim empty columns on right\n",
    "        # a tracks have an extra comma before EOL for some reason (should have max 45 columns)\n",
    "        line_list_trim_rempty.reverse()\n",
    "        line_list_trim_empty = []\n",
    "        for [i, v] in enumerate(line_list_trim_rempty):\n",
    "            if v:\n",
    "                line_list_trim_empty = line_list_trim_rempty[i:]\n",
    "                break\n",
    "        line_list_trim_empty.reverse()\n",
    "        # convert init time strings to timestamp format\n",
    "        line_list_trim_empty[2] = datetime.strptime(line_list_trim_empty[2], '%Y%m%d%H')\n",
    "            \n",
    "        track_line_lists.append(line_list_trim_empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5860e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "d823c6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the line lists to a dataframe\n",
    "df = pd.DataFrame(track_line_lists, columns=ab_column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "96c8f2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the last matching forecasts\n",
    "df_match = df.loc[(df['BASIN'] == match_basin) & (df['TECH'] == match_model)]\n",
    "\n",
    "newest_matching_cyclone_forecast_dfs = []\n",
    "for cyclone_num in df_match['CY'].unique():\n",
    "    df_cyclone = df_match.loc[(df['CY'] == cyclone_num)]\n",
    "    newest_init_time = df_cyclone['YYYYMMDDHH'].max()\n",
    "    newest_forecast = df_cyclone.loc[(df['YYYYMMDDHH'] == newest_init_time)]\n",
    "    # make sure these columns are numeric\n",
    "    # convert to numeric\n",
    "    col_numeric_names = [\"CY\", \"TAU\", \"VMAX\", \"MSLP\"]\n",
    "    for col_numeric_name in col_numeric_names:\n",
    "        newest_forecast = newest_forecast.astype({col_numeric_name:'int'})\n",
    "    # append to list of cyclone forecast dfs\n",
    "    newest_matching_cyclone_forecast_dfs.append(newest_forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b2c736",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "43f09340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL15: 0-144H : 2023-09-19 18:00:00 to 2023-09-25 00:00:00\n",
      "\n",
      "Longest forecast covers up until: 2023-09-25 00:00:00\n"
     ]
    }
   ],
   "source": [
    "import datetime as datetime\n",
    "# interpolate to provide 06 taus\n",
    "interp_intensity_cyclone_dfs = []\n",
    "\n",
    "last_valid_time = None\n",
    "\n",
    "for cyclone_df in newest_matching_cyclone_forecast_dfs:\n",
    "    forecast_taus = cyclone_df['TAU'].unique()\n",
    "    min_tau = forecast_taus.min()\n",
    "    max_tau = forecast_taus.max()\n",
    "    taus_6h = list(range(min_tau, max_tau + 6, 6))\n",
    "    taus_missing = []\n",
    "    for tau in taus_6h:\n",
    "        if tau not in forecast_taus:\n",
    "            taus_missing.append(tau)\n",
    "    for missing_tau in taus_missing:\n",
    "        # get prev tau\n",
    "        prev_tau = missing_tau\n",
    "        while prev_tau >= min_tau:\n",
    "            prev_tau = prev_tau - 1\n",
    "            if prev_tau in forecast_taus:\n",
    "                break\n",
    "        next_tau = missing_tau\n",
    "        while next_tau <= max_tau:\n",
    "            next_tau = next_tau + 1\n",
    "            if next_tau in forecast_taus:\n",
    "                break\n",
    "        prev_intensity = cyclone_df.loc[(cyclone_df['TAU'] == prev_tau)]['VMAX'].values[0]\n",
    "        next_intensity = cyclone_df.loc[(cyclone_df['TAU'] == next_tau)]['VMAX'].values[0]\n",
    "        interpolated_intensity = int(round((prev_intensity + next_intensity) / 2.0))\n",
    "        cyclone_df = cyclone_df.reset_index(drop=True)\n",
    "        prev_idx = cyclone_df.loc[(cyclone_df['TAU'] == prev_tau)].index.values[0]\n",
    "        # copy row\n",
    "        interp_row = cyclone_df.iloc[prev_idx].copy()\n",
    "        # only manipulate the TAU and INTENSITY so we don't have to copy the rest of the values\n",
    "        # the long/lat and rest of data have not been interpolated....\n",
    "        interp_row['TAU'] = missing_tau\n",
    "        interp_row['VMAX'] = interpolated_intensity\n",
    "        # add row\n",
    "        #cyclone_df = cyclone_df.append(interp_row, ignore_index=True)\n",
    "        cyclone_df = pd.concat([cyclone_df, pd.DataFrame([interp_row])], ignore_index=True)\n",
    "    cyclone_df = cyclone_df.reset_index(drop=True)\n",
    "    cyclone_df.sort_values(by=['TAU'], inplace=True)\n",
    "    cyclone_df = cyclone_df.reset_index(drop=True)\n",
    "    interp_intensity_cyclone_dfs.append(cyclone_df)\n",
    "    # cyclone number\n",
    "    cyclone_num = cyclone_df['CY'].iloc[-1]\n",
    "    basin = cyclone_df.iloc[-1]['BASIN']\n",
    "    init_time = cyclone_df.iloc[0]['YYYYMMDDHH']\n",
    "    valid_time = init_time_str + datetime.timedelta(hours=int(max_tau))\n",
    "    # print the forecast times\n",
    "    print(f\"{basin}{cyclone_num}: 0-{max_tau}H : {init_time} to {valid_time}\")\n",
    "    if last_valid_time == None:\n",
    "        last_valid_time = valid_time\n",
    "    if valid_time > last_valid_time:\n",
    "        last_valid_time = valid_time\n",
    "\n",
    "print(\"\")\n",
    "print(\"Longest forecast covers up until:\", last_valid_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "b916641f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude storms less than 35 kt\n",
    "# exclude forecasts at 00h (since this may already be included in real time ACE)\n",
    "ACE_min_kt = 35\n",
    "storm_intensities_above_min = {}\n",
    "for cyclone_df in interp_intensity_cyclone_dfs:\n",
    "    cyclone_num = cyclone_df['CY'].iloc[-1]\n",
    "    basin = cyclone_df.iloc[-1]['BASIN']\n",
    "    storm_name_str = f'{basin}{cyclone_num}'\n",
    "    min_kt_rows = cyclone_df.loc[(cyclone_df['VMAX'] >= ACE_min_kt) & (cyclone_df['TAU'] > 0)]['VMAX']\n",
    "    storm_intensities_above_min[storm_name_str] = list(min_kt_rows.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "381959c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "forecasts_ace = {}\n",
    "for [storm_name, vmax] in storm_intensities_above_min.items():\n",
    "    forecast_storm_ace = (pow(10, -4) * np.sum(np.power(vmax, 2)))\n",
    "    forecasts_ace[storm_name] = round(forecast_storm_ace,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "bdc5706e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Future contributions (up until 2023-09-25 00:00:00) of cyclone ACE values in AL basin (including INVESTs)\n",
      "    from latest forecast using IVCN (excluding 00h) by interpolating intensities\n",
      "  ACE(future) for AL15 = 9.64\n",
      "  Total ACE(future): 9.64\n",
      "\n",
      "Real-time ACE: 110.8\n",
      "Total ACE (real-time + forecast) up until 2023-09-25 00:00:00: 120.44\n"
     ]
    }
   ],
   "source": [
    "forecast_total_ace = 0\n",
    "print(f\"Future contributions (up until {last_valid_time}) of cyclone ACE values in {match_basin} basin (including INVESTs)\\n\",\n",
    "      f\"   from latest forecast using {match_model} (excluding 00h) by interpolating intensities\")\n",
    "for [storm_name, ace] in forecasts_ace.items():\n",
    "    print(f\"  ACE(future) for {storm_name} = {ace}\")\n",
    "    forecast_total_ace += ace\n",
    "print(f\"  Total ACE(future): {forecast_total_ace:4.2f}\")\n",
    "total_ace_until_forecast = real_time_ACE + forecast_total_ace\n",
    "print(\"\")\n",
    "print(\"Real-time ACE:\", real_time_ACE)\n",
    "print(f\"Total ACE (real-time + forecast) up until {last_valid_time}: {total_ace_until_forecast}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df34a6c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "74af2751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AL15': [86, 87, 86, 84, 80, 77, 74, 71, 65, 59, 52, 45, 46, 48, 49, 50, 53, 56, 55, 54, 52, 50, 51, 52]}\n"
     ]
    }
   ],
   "source": [
    "print(storm_intensities_above_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "7c656338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   BASIN  CY          YYYYMMDDHH TECHNUM/MIN  TECH  TAU LatN/S LonE/W  VMAX  \\\n",
      "0     AL  15 2023-09-19 18:00:00          03  IVCN    0     0N     0W    85   \n",
      "1     AL  15 2023-09-19 18:00:00          03  IVCN    6     0N     0W    86   \n",
      "2     AL  15 2023-09-19 18:00:00          03  IVCN   12     0N     0W    87   \n",
      "3     AL  15 2023-09-19 18:00:00          03  IVCN   18     0N     0W    86   \n",
      "4     AL  15 2023-09-19 18:00:00          03  IVCN   24     0N     0W    84   \n",
      "5     AL  15 2023-09-19 18:00:00          03  IVCN   30     0N     0W    80   \n",
      "6     AL  15 2023-09-19 18:00:00          03  IVCN   36     0N     0W    77   \n",
      "7     AL  15 2023-09-19 18:00:00          03  IVCN   42     0N     0W    74   \n",
      "8     AL  15 2023-09-19 18:00:00          03  IVCN   48     0N     0W    71   \n",
      "9     AL  15 2023-09-19 18:00:00          03  IVCN   54     0N     0W    65   \n",
      "10    AL  15 2023-09-19 18:00:00          03  IVCN   60     0N     0W    59   \n",
      "11    AL  15 2023-09-19 18:00:00          03  IVCN   66     0N     0W    52   \n",
      "12    AL  15 2023-09-19 18:00:00          03  IVCN   72     0N     0W    45   \n",
      "13    AL  15 2023-09-19 18:00:00          03  IVCN   78     0N     0W    46   \n",
      "14    AL  15 2023-09-19 18:00:00          03  IVCN   84     0N     0W    48   \n",
      "15    AL  15 2023-09-19 18:00:00          03  IVCN   90     0N     0W    49   \n",
      "16    AL  15 2023-09-19 18:00:00          03  IVCN   96     0N     0W    50   \n",
      "17    AL  15 2023-09-19 18:00:00          03  IVCN  102     0N     0W    53   \n",
      "18    AL  15 2023-09-19 18:00:00          03  IVCN  108     0N     0W    56   \n",
      "19    AL  15 2023-09-19 18:00:00          03  IVCN  114     0N     0W    55   \n",
      "20    AL  15 2023-09-19 18:00:00          03  IVCN  120     0N     0W    54   \n",
      "21    AL  15 2023-09-19 18:00:00          03  IVCN  126     0N     0W    52   \n",
      "22    AL  15 2023-09-19 18:00:00          03  IVCN  132     0N     0W    50   \n",
      "23    AL  15 2023-09-19 18:00:00          03  IVCN  138     0N     0W    51   \n",
      "24    AL  15 2023-09-19 18:00:00          03  IVCN  144     0N     0W    52   \n",
      "\n",
      "    MSLP TY RAD WINDCODE RAD1 RAD2 RAD3 RAD4 POUTER ROUTER   RMW GUSTS   EYE  \\\n",
      "0      0      0             0    0    0    0   None   None  None  None  None   \n",
      "1      0      0             0    0    0    0   None   None  None  None  None   \n",
      "2      0      0             0    0    0    0   None   None  None  None  None   \n",
      "3      0      0             0    0    0    0   None   None  None  None  None   \n",
      "4      0      0             0    0    0    0   None   None  None  None  None   \n",
      "5      0      0             0    0    0    0   None   None  None  None  None   \n",
      "6      0      0             0    0    0    0   None   None  None  None  None   \n",
      "7      0      0             0    0    0    0   None   None  None  None  None   \n",
      "8      0      0             0    0    0    0   None   None  None  None  None   \n",
      "9      0      0             0    0    0    0   None   None  None  None  None   \n",
      "10     0      0             0    0    0    0   None   None  None  None  None   \n",
      "11     0      0             0    0    0    0   None   None  None  None  None   \n",
      "12     0      0             0    0    0    0   None   None  None  None  None   \n",
      "13     0      0             0    0    0    0   None   None  None  None  None   \n",
      "14     0      0             0    0    0    0   None   None  None  None  None   \n",
      "15     0      0             0    0    0    0   None   None  None  None  None   \n",
      "16     0      0             0    0    0    0   None   None  None  None  None   \n",
      "17     0      0             0    0    0    0   None   None  None  None  None   \n",
      "18     0      0             0    0    0    0   None   None  None  None  None   \n",
      "19     0      0             0    0    0    0   None   None  None  None  None   \n",
      "20     0      0             0    0    0    0   None   None  None  None  None   \n",
      "21     0      0             0    0    0    0   None   None  None  None  None   \n",
      "22     0      0             0    0    0    0   None   None  None  None  None   \n",
      "23     0      0             0    0    0    0   None   None  None  None  None   \n",
      "24     0      0             0    0    0    0   None   None  None  None  None   \n",
      "\n",
      "   SUBREGION MAXSEAS INITIALS   DIR SPEED STORMNAME DEPTH  SEAS SEASCODE  \\\n",
      "0       None    None     None  None  None      None  None  None     None   \n",
      "1       None    None     None  None  None      None  None  None     None   \n",
      "2       None    None     None  None  None      None  None  None     None   \n",
      "3       None    None     None  None  None      None  None  None     None   \n",
      "4       None    None     None  None  None      None  None  None     None   \n",
      "5       None    None     None  None  None      None  None  None     None   \n",
      "6       None    None     None  None  None      None  None  None     None   \n",
      "7       None    None     None  None  None      None  None  None     None   \n",
      "8       None    None     None  None  None      None  None  None     None   \n",
      "9       None    None     None  None  None      None  None  None     None   \n",
      "10      None    None     None  None  None      None  None  None     None   \n",
      "11      None    None     None  None  None      None  None  None     None   \n",
      "12      None    None     None  None  None      None  None  None     None   \n",
      "13      None    None     None  None  None      None  None  None     None   \n",
      "14      None    None     None  None  None      None  None  None     None   \n",
      "15      None    None     None  None  None      None  None  None     None   \n",
      "16      None    None     None  None  None      None  None  None     None   \n",
      "17      None    None     None  None  None      None  None  None     None   \n",
      "18      None    None     None  None  None      None  None  None     None   \n",
      "19      None    None     None  None  None      None  None  None     None   \n",
      "20      None    None     None  None  None      None  None  None     None   \n",
      "21      None    None     None  None  None      None  None  None     None   \n",
      "22      None    None     None  None  None      None  None  None     None   \n",
      "23      None    None     None  None  None      None  None  None     None   \n",
      "24      None    None     None  None  None      None  None  None     None   \n",
      "\n",
      "   SEAS1 SEAS2 SEAS3 SEAS4 USERDEFINED1 userdata1 USERDEFINED2 userdata2  \\\n",
      "0   None  None  None  None         None      None         None      None   \n",
      "1   None  None  None  None         None      None         None      None   \n",
      "2   None  None  None  None         None      None         None      None   \n",
      "3   None  None  None  None         None      None         None      None   \n",
      "4   None  None  None  None         None      None         None      None   \n",
      "5   None  None  None  None         None      None         None      None   \n",
      "6   None  None  None  None         None      None         None      None   \n",
      "7   None  None  None  None         None      None         None      None   \n",
      "8   None  None  None  None         None      None         None      None   \n",
      "9   None  None  None  None         None      None         None      None   \n",
      "10  None  None  None  None         None      None         None      None   \n",
      "11  None  None  None  None         None      None         None      None   \n",
      "12  None  None  None  None         None      None         None      None   \n",
      "13  None  None  None  None         None      None         None      None   \n",
      "14  None  None  None  None         None      None         None      None   \n",
      "15  None  None  None  None         None      None         None      None   \n",
      "16  None  None  None  None         None      None         None      None   \n",
      "17  None  None  None  None         None      None         None      None   \n",
      "18  None  None  None  None         None      None         None      None   \n",
      "19  None  None  None  None         None      None         None      None   \n",
      "20  None  None  None  None         None      None         None      None   \n",
      "21  None  None  None  None         None      None         None      None   \n",
      "22  None  None  None  None         None      None         None      None   \n",
      "23  None  None  None  None         None      None         None      None   \n",
      "24  None  None  None  None         None      None         None      None   \n",
      "\n",
      "   USERDEFINED3 userdata3 USERDEFINED4 userdata4 USERDEFINED5 userdata5  \n",
      "0          None      None         None      None         None      None  \n",
      "1          None      None         None      None         None      None  \n",
      "2          None      None         None      None         None      None  \n",
      "3          None      None         None      None         None      None  \n",
      "4          None      None         None      None         None      None  \n",
      "5          None      None         None      None         None      None  \n",
      "6          None      None         None      None         None      None  \n",
      "7          None      None         None      None         None      None  \n",
      "8          None      None         None      None         None      None  \n",
      "9          None      None         None      None         None      None  \n",
      "10         None      None         None      None         None      None  \n",
      "11         None      None         None      None         None      None  \n",
      "12         None      None         None      None         None      None  \n",
      "13         None      None         None      None         None      None  \n",
      "14         None      None         None      None         None      None  \n",
      "15         None      None         None      None         None      None  \n",
      "16         None      None         None      None         None      None  \n",
      "17         None      None         None      None         None      None  \n",
      "18         None      None         None      None         None      None  \n",
      "19         None      None         None      None         None      None  \n",
      "20         None      None         None      None         None      None  \n",
      "21         None      None         None      None         None      None  \n",
      "22         None      None         None      None         None      None  \n",
      "23         None      None         None      None         None      None  \n",
      "24         None      None         None      None         None      None  ]\n"
     ]
    }
   ],
   "source": [
    "print(interp_intensity_cyclone_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e287a753",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "6ced0a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Define a function to convert the components into datetime objects\n",
    "def convert_to_datetime(components, debug = False):\n",
    "    # only use first time\n",
    "    try:\n",
    "        c = \"\".join(components.astype(str))\n",
    "        year = int(c[0:4])\n",
    "        month = int(c[5:7])\n",
    "        day = int(c[8:10])\n",
    "        hour = int(c[11:13])\n",
    "        minute = int(c[14:16])\n",
    "        second = int(c[17:])\n",
    "        dt = datetime(year, month, day, hour, minute, second)\n",
    "        return dt\n",
    "    except:\n",
    "        if debug:\n",
    "            print(\"err\")\n",
    "            print(components)\n",
    "        return None\n",
    "    \n",
    "def extract_first_num(components):\n",
    "    try:\n",
    "        return components[0].astype(int)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def extract_first_str(components):\n",
    "    try:\n",
    "        return \"\".join(components[0].astype(str))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def extract_first_byte(components):\n",
    "    try:\n",
    "        return \"\".join(components[0].astype(str))\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "c4735dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate historical ACE energies from best tracks and use this to calculate expected ACE values from the above values\n",
    "\n",
    "# from https://www.ncei.noaa.gov/data/international-best-track-archive-for-climate-stewardship-ibtracs/v04r00/access/csv/\n",
    "best_tracks_csv = 'ibtracs.NA.list.v04r00.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame without automatically assigning column names\n",
    "df = pd.read_csv(best_tracks_csv, header=0, skiprows=[1], low_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "ad2d4271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary that specifies the data types for each column\n",
    "dtype_mapping = {\n",
    "    'LAT': float,\n",
    "    'LON': float,\n",
    "    'WMO_WIND': float,\n",
    "    'WMO_PRES': float,\n",
    "    'DIST2LAND': float,\n",
    "    'LANDFALL': float,\n",
    "    'STORM_SPEED': float,\n",
    "    'STORM_DIR': float\n",
    "}\n",
    "\n",
    "# Apply dtype conversion to specific columns, treating non-numeric values as NaN\n",
    "for column, data_type in dtype_mapping.items():\n",
    "    df[column] = pd.to_numeric(df[column], errors='coerce').astype(data_type)\n",
    "\n",
    "# convert time to datetime\n",
    "df['ISO_TIME'] = pd.to_datetime(df['ISO_TIME'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "eb3d9ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a mask to filter the rows for only 6 hour times\n",
    "mask = (df['ISO_TIME'].dt.hour.isin([0, 6, 12, 18])) & (df['ISO_TIME'].dt.minute == 0) & (df['ISO_TIME'].dt.second == 0)\n",
    "\n",
    "# Apply the mask to filter the DataFrame\n",
    "filtered_df = df[mask].copy()\n",
    "\n",
    "# drop all but relevant column's for speed\n",
    "selected_columns = ['SEASON', 'ISO_TIME', 'WMO_WIND']\n",
    "filtered_df = filtered_df[selected_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "25082eea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>SEASON</th>\n",
       "      <th>ISO_TIME</th>\n",
       "      <th>WMO_WIND</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>1851</td>\n",
       "      <td>1851-06-25 00:00:00</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>1851</td>\n",
       "      <td>1851-06-25 06:00:00</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>1851</td>\n",
       "      <td>1851-06-25 12:00:00</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>1851</td>\n",
       "      <td>1851-06-25 18:00:00</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>1851</td>\n",
       "      <td>1851-06-26 00:00:00</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40176</th>\n",
       "      <td>125434</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022-11-10 00:00:00</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40177</th>\n",
       "      <td>125436</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022-11-10 06:00:00</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40178</th>\n",
       "      <td>125439</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022-11-10 12:00:00</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40179</th>\n",
       "      <td>125441</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022-11-10 18:00:00</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40180</th>\n",
       "      <td>125444</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022-11-11 00:00:00</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40181 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index  SEASON            ISO_TIME  WMO_WIND\n",
       "0          12    1851 1851-06-25 00:00:00      80.0\n",
       "1          14    1851 1851-06-25 06:00:00      80.0\n",
       "2          16    1851 1851-06-25 12:00:00      80.0\n",
       "3          18    1851 1851-06-25 18:00:00      80.0\n",
       "4          20    1851 1851-06-26 00:00:00      70.0\n",
       "...       ...     ...                 ...       ...\n",
       "40176  125434    2022 2022-11-10 00:00:00      65.0\n",
       "40177  125436    2022 2022-11-10 06:00:00      65.0\n",
       "40178  125439    2022 2022-11-10 12:00:00      55.0\n",
       "40179  125441    2022 2022-11-10 18:00:00      40.0\n",
       "40180  125444    2022 2022-11-11 00:00:00      35.0\n",
       "\n",
       "[40181 rows x 4 columns]"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_min_35kt = filtered_df.loc[(filtered_df['WMO_WIND'] >= 35)]\n",
    "df_min_35kt.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e7e6ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9fd0e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "e2aeaf8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done calculating ACE for each year up until 9/25\n"
     ]
    }
   ],
   "source": [
    "#### if desired, use a manually specified time (must comment out the below line using last_valid_time)\n",
    "ace_time_end_str = \"2023-09-24 06:00:00\"\n",
    "ace_time = datetime.fromisoformat(ace_time_end_str)\n",
    "\n",
    "# otherwise, use the last valid_time\n",
    "ace_time = last_valid_time\n",
    "####\n",
    "\n",
    "\n",
    "# Calculate ACE for up to ace_time_end_str for all years\n",
    "\n",
    "ace_time_month = ace_time.month\n",
    "ace_time_day = ace_time.day\n",
    "\n",
    "season_ace = {}\n",
    "season_kt = {}\n",
    "kt_at_time = {}\n",
    "season_ace_to_time_end = {}\n",
    "season_ace_above_real_time = {}\n",
    "\n",
    "for idx in range(0, len(df_min_35kt)):\n",
    "    r = df_min_35kt.iloc[idx]\n",
    "    t = r['ISO_TIME']\n",
    "    w = r['WMO_WIND']\n",
    "    kt_at_time[t] = w\n",
    "\n",
    "years = list(set(list(df_min_35kt['SEASON'].values)))\n",
    "    \n",
    "for season in years:\n",
    "    df_season_min_35kt = df_min_35kt.loc[(df_min_35kt['SEASON'] == season)]\n",
    "    kts = list(df_season_min_35kt['WMO_WIND'].values)\n",
    "    season_kt[season] = kts\n",
    "    ace = (pow(10, -4) * np.sum(np.power(kts, 2)))\n",
    "    season_ace[season] = ace\n",
    "    \n",
    "    # exclude ace that are not representative for population statistics when they are below real time ACE\n",
    "    if ace >= real_time_ACE:\n",
    "        season_ace_above_real_time[season] = ace\n",
    "    \n",
    "    # calculate ACE until ace_time_end_str for each year\n",
    "    df_season_to_time_end_min_35kt = df_season_min_35kt.loc[\n",
    "        (df_season_min_35kt['ISO_TIME'].dt.month < ace_time_month) | \n",
    "        (\n",
    "            (df_season_min_35kt['ISO_TIME'].dt.month == ace_time_month) &\n",
    "            (df_season_min_35kt['ISO_TIME'].dt.day <= ace_time_day)\n",
    "        )\n",
    "    ]\n",
    "    kts = list(df_season_to_time_end_min_35kt['WMO_WIND'].values)\n",
    "    ace = (pow(10, -4) * np.sum(np.power(kts, 2)))\n",
    "    season_ace_to_time_end[season] = ace\n",
    "\n",
    "print(f\"Done calculating ACE for each year up until {last_valid_time.month}/{last_valid_time.day}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a27f25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60399e5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "6ea9f06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this to normalize a 1-d array\n",
    "def normalize(matrix):\n",
    "    norm = np.linalg.norm(matrix, 1)\n",
    "    matrix = matrix/norm \n",
    "    return matrix\n",
    "\n",
    "# use exponential decay (the difference passed should always be positive)\n",
    "def custom_kernel(difference, rate):\n",
    "    #return (1.0 / difference)\n",
    "    return np.exp(-rate * difference)\n",
    "\n",
    "def get_min_pair_by_value(d):\n",
    "    # Find the key/value pair with the minimum value from a dict\n",
    "    return min(d.items(), key=lambda x: x[1])\n",
    "\n",
    "def mean_squared_error(predictions, actual_values):\n",
    "    # Ensure both dictionaries have the same keys\n",
    "    common_keys = set(predictions.keys()) & set(actual_values.keys())\n",
    "    \n",
    "    # Calculate the squared differences and sum them\n",
    "    squared_diff_sum = sum((predictions[key] - actual_values[key])**2 for key in common_keys)\n",
    "    \n",
    "    # Calculate the mean squared error\n",
    "    mse = squared_diff_sum / len(common_keys)\n",
    "    \n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee59b6bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "c5b97a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning years:  [1851, 2015, 1864, 1867, 1976, 2009, 1955, 1969, 1880, 1881, 1897, 1988, 1919, 2000, 2006, 1877, 1909, 2012, 1920, 1981, 1972, 2019, 1934, 1974, 1912, 1868, 1876, 1893, 1905, 1921, 1858, 1855, 1900, 1989, 1966, 1859, 2007, 1856, 1971, 1964, 1960, 2022, 1942, 1983, 1949, 1973, 2020, 1987, 1889, 2021, 1894, 1998, 1910, 1903, 1870, 1872, 1924, 1861, 1852, 1967, 1931, 1918, 1865, 1984, 1928, 1887, 1944, 2010, 1917, 1891, 1886, 1907, 1927, 1908, 1993, 1898, 1991, 2001, 1954, 1947, 1914, 1922, 1994, 2005, 1963, 1890, 1956, 1896, 1916, 1871, 1936, 1970, 1979, 1913, 1929, 1935, 1996, 2016, 1892, 1926, 1869, 1938, 2003, 1883, 1901, 1888, 1933, 1985, 2002, 1992, 1882, 1977, 1911, 1904, 1857, 1878, 1990, 1957, 1980, 1902, 1958, 1884, 1930, 1943, 2014, 1925, 1951, 1965, 1986, 1874, 1940, 2008, 2013, 2018, 1968, 1999, 1952]\n",
      "Validation years:  [1982, 1879, 1899, 1860, 1978, 1995, 1885, 2004, 1961, 1862, 1866, 1941, 1962, 1873, 1950, 1932, 1895, 1946, 2017, 1939, 1915, 1997, 1923, 1863, 1953, 1854, 1948, 1937, 1906, 1975, 1945, 1959, 2011, 1875, 1853]\n"
     ]
    }
   ],
   "source": [
    "### This should be done regularly (after every day?) to get lower variance and proper decay values considering the time until to the end of the season\n",
    "\n",
    "### TUNING & VALIDATION for decay rates and expected ACE (for this far out in year) ###\n",
    "\n",
    "### do tuning on a partial dataset to calculate MSE and then do validation on the remaining dataset\n",
    "### to make sure MSE is not too far off (this will be an estimate of the additional error/uncertainty in the tuning)\n",
    "\n",
    "import random\n",
    "\n",
    "years = list(season_ace_to_time_end.keys())\n",
    "years_above_real_time_ace = sorted(list(season_ace_above_real_time.keys()))\n",
    "\n",
    "# Calculate the size of the first list (80%)\n",
    "tuning_list_size = int(len(years) * 0.8)\n",
    "\n",
    "# Randomly shuffle the original list\n",
    "random.shuffle(years)\n",
    "\n",
    "# Split the list into two new lists\n",
    "years_tuning = years[:tuning_list_size]\n",
    "years_validation = years[tuning_list_size:]\n",
    "print(f\"Tuning years: \", years_tuning)\n",
    "print(f\"Validation years: \", years_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "b22b1e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exponential decay weights\n",
    "rates_to_try = list(np.round(np.arange(0.1, 0.2, 0.001),3))\n",
    "#rates_to_try = [0.0001, 0.001, 0.01, 0.1, 1]\n",
    "mse_by_rate = {}\n",
    "\n",
    "# Calculate weights for each decay rate\n",
    "for rate in rates_to_try:\n",
    "    expected_ACE_by_tuning_year = {}\n",
    "    # tune over only years for validation set\n",
    "    for tuning_year in years_tuning:\n",
    "        # for tuning this is the season_ace_to_time_end[tuning_year]\n",
    "        tuning_ACE = season_ace_to_time_end[tuning_year]\n",
    "\n",
    "        tuning_diff_season_ace_to_time_end = {}\n",
    "\n",
    "        for [year, ace] in season_ace_to_time_end.items():\n",
    "            if year in years_tuning and year != tuning_year:\n",
    "                tuning_diff_season_ace_to_time_end[year] = tuning_ACE - ace\n",
    "\n",
    "        # Calculate weights for each year\n",
    "        weights_normalized = []\n",
    "        weights = []\n",
    "        for [year, diff] in tuning_diff_season_ace_to_time_end.items():        \n",
    "            # Calculate weights using the custom kernel function\n",
    "            weight = custom_kernel(np.abs(diff), rate)\n",
    "            weights.append(weight)\n",
    "\n",
    "        # Calculate the normalized weights\n",
    "        weights_normalized = normalize(weights)\n",
    "\n",
    "        # Calculate expected ACE using normalized weights\n",
    "        expected_ACE = 0.0\n",
    "        for [x, year] in list(enumerate(tuning_diff_season_ace_to_time_end)):\n",
    "            expected_ACE += (weights_normalized[x] * season_ace[year])\n",
    "        #print(tuning_diff_season_ace_to_time_end)\n",
    "\n",
    "        expected_ACE_by_tuning_year[tuning_year] = expected_ACE\n",
    "    \n",
    "    # calculate the MSE for this rate\n",
    "    #print(expected_ACE_by_tuning_year)\n",
    "    mse = mean_squared_error(expected_ACE_by_tuning_year, season_ace)\n",
    "    mse_by_rate[rate] = mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "bb561d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Validation) Rate , MSE (minimum) for tuning dataset: (0.136, 525.5818424841025)\n"
     ]
    }
   ],
   "source": [
    "tuning_min_pair = get_min_pair_by_value(mse_by_rate)\n",
    "print(f\"(Validation) Rate , MSE (minimum) for tuning dataset: {tuning_min_pair}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "73f4e90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TUNING above has minimum MSE for:\n",
    "# Rate: 0.14, MSE: 578.8630529802942"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "674e1fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## VALIDATION\n",
    "# exponential decay weights\n",
    "rates_to_try = list(np.round(np.arange(0.05, 0.2, 0.001),3))\n",
    "#rates_to_try = [0.0001, 0.001, 0.01, 0.1, 1]\n",
    "mse_by_rate = {}\n",
    "\n",
    "# Calculate weights for each decay rate\n",
    "for rate in rates_to_try:\n",
    "    expected_ACE_by_validation_year = {}\n",
    "    for validation_year in years_validation:\n",
    "        # for validation this is the season_ace_to_time_end[validation_year]\n",
    "        validation_ACE = season_ace_to_time_end[validation_year]\n",
    "\n",
    "        validation_diff_season_ace_to_time_end = {}\n",
    "\n",
    "        for [year, ace] in season_ace_to_time_end.items():\n",
    "            if year in years_validation and year != validation_year:\n",
    "                validation_diff_season_ace_to_time_end[year] = validation_ACE - ace\n",
    "\n",
    "        # Calculate weights for each year\n",
    "        weights_normalized = []\n",
    "        weights = []\n",
    "        for [year, diff] in validation_diff_season_ace_to_time_end.items():        \n",
    "            # Calculate weights using the custom kernel function\n",
    "            weight = custom_kernel(np.abs(diff), rate)\n",
    "            weights.append(weight)\n",
    "\n",
    "        # Calculate the normalized weights\n",
    "        weights_normalized = normalize(weights)\n",
    "\n",
    "        # Calculate expected ACE using normalized weights\n",
    "        expected_ACE = 0.0\n",
    "        for [x, year] in list(enumerate(validation_diff_season_ace_to_time_end)):\n",
    "            expected_ACE += (weights_normalized[x] * season_ace[year])\n",
    "        #print(validation_diff_season_ace_to_time_end)\n",
    "\n",
    "        expected_ACE_by_validation_year[validation_year] = expected_ACE\n",
    "    \n",
    "    # calculate the MSE for this rate\n",
    "    #print(expected_ACE_by_validation_year)\n",
    "    mse = mean_squared_error(expected_ACE_by_validation_year, season_ace)\n",
    "    mse_by_rate[rate] = mse\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "5c017339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Validation) Rate : MSE (minimum) for tuning dataset: (0.091, 417.0722495923594)\n"
     ]
    }
   ],
   "source": [
    "validation_min_pair = get_min_pair_by_value(mse_by_rate)\n",
    "print(f\"(Validation) Rate : MSE (minimum) for tuning dataset: {validation_min_pair}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "3c5ae2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE tuning: 22.925571802773042\n",
      "RMSE validation: 22.925571802773042\n",
      "Difference of RMSE(tuning) - RMSE(validation): 1.5821583308412621\n"
     ]
    }
   ],
   "source": [
    "# for validation, the minimum MSE:\n",
    "# Rate: 0.102, MSE: 439.9012525009955\n",
    "# from the validation-tuning set above, the tuning Rate: 0.14, MSE: 450.0207460685012\n",
    "\n",
    "rmse_tuning = pow(tuning_min_pair[1], 0.5)\n",
    "rmse_validation = pow(validation_min_pair[1], 0.5)\n",
    "diff_rmse = abs(rmse_tuning - rmse_validation)\n",
    "print(\"RMSE tuning:\", rmse_tuning)\n",
    "print(\"RMSE validation:\", rmse_tuning)                 \n",
    "print(\"Difference of RMSE(tuning) - RMSE(validation):\", pow(diff_rmse,0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83427dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "4864ff93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Tuning Final) Rate : MSE (minimum) for all past years: (0.171, 481.5783055586021)\n"
     ]
    }
   ],
   "source": [
    "# FINAL TUNING\n",
    "\n",
    "# exponential decay weights\n",
    "rates_to_try = list(np.round(np.arange(0.05, 0.2, 0.001),3))\n",
    "#rates_to_try = [0.0001, 0.001, 0.01, 0.1, 1]\n",
    "mse_by_rate = {}\n",
    "\n",
    "# Calculate weights for each decay rate\n",
    "for rate in rates_to_try:\n",
    "    expected_ACE_by_tuning_year = {}\n",
    "    # tune over all years\n",
    "    for tuning_year in years:\n",
    "        # for tuning this is the season_ace_to_time_end[tuning_year]\n",
    "        tuning_ACE = season_ace_to_time_end[tuning_year]\n",
    "\n",
    "        tuning_diff_season_ace_to_time_end = {}\n",
    "\n",
    "        for [year, ace] in season_ace_to_time_end.items():\n",
    "            if year in years and year != tuning_year:\n",
    "                tuning_diff_season_ace_to_time_end[year] = tuning_ACE - ace\n",
    "\n",
    "        # Calculate weights for each year\n",
    "        weights_normalized = []\n",
    "        weights = []\n",
    "        for [year, diff] in tuning_diff_season_ace_to_time_end.items():        \n",
    "            # Calculate weights using the custom kernel function\n",
    "            weight = custom_kernel(np.abs(diff), rate)\n",
    "            weights.append(weight)\n",
    "\n",
    "        # Calculate the normalized weights\n",
    "        weights_normalized = normalize(weights)\n",
    "\n",
    "        # Calculate expected ACE using normalized weights\n",
    "        expected_ACE = 0.0\n",
    "        for [x, year] in list(enumerate(tuning_diff_season_ace_to_time_end)):\n",
    "            expected_ACE += (weights_normalized[x] * season_ace[year])\n",
    "        #print(tuning_diff_season_ace_to_time_end)\n",
    "\n",
    "        expected_ACE_by_tuning_year[tuning_year] = expected_ACE\n",
    "    \n",
    "    # calculate the MSE for this rate\n",
    "    #print(expected_ACE_by_tuning_year)\n",
    "    mse = mean_squared_error(expected_ACE_by_tuning_year, season_ace)\n",
    "    mse_by_rate[rate] = mse\n",
    "    #print(f\"Rate: {rate}, MSE: {mse}\")\n",
    "\n",
    "final_tuning_min_pair = get_min_pair_by_value(mse_by_rate)\n",
    "print(f\"(Tuning Final) Rate : MSE (minimum) for all past years: {final_tuning_min_pair}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "4bf85392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate final tuning: 0.171\n",
      "MSE final tuning: 481.5783055586021\n",
      "RMSE final tuning: 21.777957351839856\n"
     ]
    }
   ],
   "source": [
    "# minimum is Rate: 0.147, MSE: 518.519969751831\n",
    "mse_final_tuning = final_tuning_min_pair[1]\n",
    "rate_final_tuning = final_tuning_min_pair[0]\n",
    "print(\"Rate final tuning:\", rate_final_tuning)\n",
    "print(\"MSE final tuning:\", mse_final_tuning)\n",
    "rmse_final_tuning = pow(mse_tuning,0.5)\n",
    "print(\"RMSE final tuning:\", var_final_tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3581c13d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766a8f51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9d1ee3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "0d2411f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecasts until: 2023-09-25 00:00:00\n",
      "ACE, Real-time + storm model forecast: 120.44\n",
      "Tuned decay rate: 0.171\n",
      "\n",
      "Expected ACE (weight historical ACE using exponentially decay): 168.00\n",
      "(bootstrapped) CI using expected value (70.0%): [160.48, 237.50]\n",
      "\n",
      "Statistics for 1851 - 2022 excluding years with season ACE < realtime ACE:\n",
      "μ: 164.27\n",
      "(bootstrapped) CI using mean (54.0%): [160.02, 168.31]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "from collections import Counter\n",
    "\n",
    "# Calculate the critical value for the specified confidence level\n",
    "critical_value = norm.ppf((1 + confidence_level) / 2)\n",
    "\n",
    "# real-time ACE plus forecast ace (until ace_time_end_str)\n",
    "# use a manually entered value\n",
    "present_ACE = 122.65\n",
    "# or the value from the forecast\n",
    "present_ACE = total_ace_until_forecast\n",
    "\n",
    "def calculate_expected_ACE(this_season_end = season_ace_to_time_end, repeat_counts = None):\n",
    "    diff_season_ace_to_time_end = {}\n",
    "\n",
    "    for [year, ace] in this_season_end.items():\n",
    "        diff_season_ace_to_time_end[year] = present_ACE - ace\n",
    "\n",
    "    # exponential decay weights\n",
    "    #rates_to_try = list(np.round(np.arange(0.080, 0.205, 0.005),3))\n",
    "    rates_to_try = [rate_final_tuning]\n",
    "\n",
    "    expected_ACE_by_rate = {}\n",
    "    weights_normalized_by_rate = {}\n",
    "    # Calculate weights for each decay rate\n",
    "    for rate in rates_to_try:\n",
    "        # Calculate weights for each year\n",
    "        weights_normalized = []\n",
    "        weights = []\n",
    "        for [year, diff] in diff_season_ace_to_time_end.items():        \n",
    "            # Calculate weights using the custom kernel function\n",
    "            weight = custom_kernel(np.abs(diff), rate)\n",
    "            weights.append(weight)\n",
    "\n",
    "        # Calculate the normalized weights\n",
    "        weights_normalized = normalize(weights)\n",
    "\n",
    "        weights_normalized_by_rate[rate] = weights_normalized\n",
    "\n",
    "        # Calculate expected ACE using normalized weights\n",
    "        expected_ACE = 0.0\n",
    "        for [x, year] in list(enumerate(diff_season_ace_to_time_end)):\n",
    "            if repeat_counts:\n",
    "                repeat_count = repeat_counts[year]\n",
    "                expected_ACE += (repeat_count * (weights_normalized[x] * season_ace[year]))\n",
    "            else:\n",
    "                expected_ACE += (weights_normalized[x] * season_ace[year])\n",
    "\n",
    "        expected_ACE_by_rate[rate] = expected_ACE\n",
    "\n",
    "    expected_ACE = expected_ACE_by_rate[rate_final_tuning]\n",
    "\n",
    "    return expected_ACE\n",
    "\n",
    "\n",
    "expected_ACE = calculate_expected_ACE()\n",
    "    \n",
    "pop_ace_values = list(season_ace_above_real_time.values())\n",
    "ace_stddev = np.std(pop_ace_values)\n",
    "ace_mean =  np.mean(pop_ace_values)\n",
    "z_score = (expected_ACE - ace_mean) / ace_stddev\n",
    "\n",
    "\n",
    "sorted_years = sorted(years)\n",
    "\n",
    "print(f\"Forecasts until:\", last_valid_time)\n",
    "print(f\"ACE, Real-time + storm model forecast: {total_ace_until_forecast}\")\n",
    "print(\"Tuned decay rate:\", rate_final_tuning)\n",
    "print(\"\")\n",
    "print(f\"Expected ACE (weight historical ACE using exponentially decay): {expected_ACE:3.2f}\")\n",
    "\n",
    "\n",
    "# Specify the confidence level (e.g., 90% CI)\n",
    "confidence_level = 0.7\n",
    "\n",
    "# use bootstrapping instead for CI\n",
    "\n",
    "# Number of bootstrap samples to generate\n",
    "num_samples = 10000\n",
    "\n",
    "# Calculate the statistic of interest (e.g., mean) for the original data\n",
    "original_statistic = expected_ACE\n",
    "\n",
    "# Initialize an array to store resampled statistics\n",
    "resampled_statistics = np.empty(num_samples)\n",
    "\n",
    "# Perform bootstrapping\n",
    "for i in range(num_samples):\n",
    "    # Generate a resampled dataset by sampling with replacement\n",
    "    \n",
    "    # cannot do random choice on a dict...\n",
    "    #resampled_data = np.random.choice(season_ace_above_real_time, size=len(season_ace_above_real_time), replace=True)\n",
    "    \n",
    "    num_samples = len(season_ace_above_real_time)  # Number of samples you want to draw\n",
    "    # Randomly select keys from the dictionary with replacement\n",
    "    random_keys = random.choices(list(season_ace_to_time_end.keys()), k=num_samples)\n",
    "    # Use the selected keys to access corresponding values\n",
    "    random_values = [season_ace_to_time_end[key] for key in random_keys]\n",
    "    \n",
    "    # since I need to pass a dict that should have multiple keys repeated, do something extra to keep track of the 'repeats'\n",
    "    repeat_counts = dict(Counter(random_keys))\n",
    "    \n",
    "    resampled_data = {key: value for key, value in zip(random_keys, random_values)}\n",
    "    \n",
    "    # Calculate the statistic of interest for the resampled dataset\n",
    "    resampled_statistic = calculate_expected_ACE(resampled_data, repeat_counts)\n",
    "    \n",
    "    # Store the resampled statistic\n",
    "    resampled_statistics[i] = resampled_statistic\n",
    "\n",
    "# Calculate the lower and upper bounds of the confidence interval\n",
    "alpha = 1 - confidence_level\n",
    "lower_bound = np.percentile(resampled_statistics, 100 * alpha / 2)\n",
    "upper_bound = np.percentile(resampled_statistics, 100 * (1 - alpha / 2))\n",
    "\n",
    "# Print the results\n",
    "print(f\"(bootstrapped) CI using expected value ({confidence_level*100:2.1f}%): [{lower_bound:3.2f}, {upper_bound:3.2f}]\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Specify the confidence level (e.g., 90% CI)\n",
    "confidence_level = 0.54\n",
    "\n",
    "print(\"\")\n",
    "print(f\"Statistics for {sorted_years[0]} - {sorted_years[-1]} excluding years with season ACE < realtime ACE:\")\n",
    "print(f\"μ: {ace_mean:3.2f}\")\n",
    "\n",
    "# not as useful since it doesn't handle constraints well so don't use general statistics below for CI:\n",
    "#print(\"σ:\", ace_stddev)\n",
    "#print(\"Z score for expected ACE:\", z_score)\n",
    "#ci_lower = expected_ACE - (critical_value * ace_stddev)\n",
    "#ci_upper = expected_ACE + (critical_value * ace_stddev)\n",
    "#print(f\"CI({confidence_level}) = [ {ci_lower} , {ci_upper} ]\")\n",
    "\n",
    "\n",
    "# use bootstrapping instead for CI\n",
    "\n",
    "# Number of bootstrap samples to generate\n",
    "num_samples = 10000\n",
    "\n",
    "# Calculate the statistic of interest (e.g., mean) for the original data\n",
    "original_statistic = ace_mean\n",
    "\n",
    "# Initialize an array to store resampled statistics\n",
    "resampled_statistics = np.empty(num_samples)\n",
    "\n",
    "# Perform bootstrapping\n",
    "for i in range(num_samples):\n",
    "    # Generate a resampled dataset by sampling with replacement\n",
    "    resampled_data = np.random.choice(pop_ace_values, size=len(pop_ace_values), replace=True)\n",
    "    \n",
    "    # Calculate the statistic of interest for the resampled dataset\n",
    "    resampled_statistic = np.mean(resampled_data)\n",
    "    \n",
    "    # Store the resampled statistic\n",
    "    resampled_statistics[i] = resampled_statistic\n",
    "\n",
    "# Calculate the lower and upper bounds of the confidence interval\n",
    "alpha = 1 - confidence_level\n",
    "lower_bound = np.percentile(resampled_statistics, 100 * alpha / 2)\n",
    "upper_bound = np.percentile(resampled_statistics, 100 * (1 - alpha / 2))\n",
    "\n",
    "# Print the results\n",
    "print(f\"(bootstrapped) CI using mean ({confidence_level*100:2.1f}%): [{lower_bound:3.2f}, {upper_bound:3.2f}]\")\n",
    "\n",
    "\n",
    "# expect the tuned decay rate should automatically increase as the season passes\n",
    "#   this means, later in the season it will be more selective in using years that have ACE that are very close to the present\n",
    "#   as opposed to early in the season when the years will be more of an average\n",
    "# for instance on Sept 24 06Z it was 0.147, and on Sept 25 00Z it was 0.171\n",
    "print(\"\")\n",
    "\n",
    "# diagnostics for model predictiveness as the season progresses\n",
    "expected_ACE_upper = expected_ACE + rmse_final_tuning\n",
    "expected_ACE_lower = expected_ACE - rmse_final_tuning\n",
    "#print(f\"Range using RMSE (without validation RMSE): {expected_ACE_lower} : {expected_ACE_upper}\")\n",
    "expected_ACE_upper_with_validation = expected_ACE + rmse_final_tuning + diff_rmse\n",
    "expected_ACE_lower_with_validation = expected_ACE - rmse_final_tuning - diff_rmse\n",
    "#print(f\"Range using RMSE (with validation RMSE):    {expected_ACE_lower_with_validation} : {expected_ACE_upper_with_validation}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607601b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b91aa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393d6bab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

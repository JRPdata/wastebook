{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "3fda4046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download from gefs-aws into split gribs using curl\n",
    "# lot of hard coding needs to be changed to customize\n",
    "# see cell with # HARDCODED REGEX\n",
    "\n",
    "\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "\n",
    "# CONFIG\n",
    "bucket_name = \"noaa-gefs-pds\"\n",
    "prefix = 'gefs.202406'\n",
    "max_hour = 384\n",
    "\n",
    "model_url_folder = 'https://noaa-gefs-pds.s3.amazonaws.com/'\n",
    "model_data_folder = '/home/db/Documents/JRPdata/hot-july/gefs'\n",
    "# the identifiable string in the .idx files to extract split gribs (should be in format ':shortname:desc:')\n",
    "params = [':TMP:2 m above ground:']\n",
    "# how long to pause between downloads as to not hammer server\n",
    "sleep_time = 1\n",
    "\n",
    "s3_client = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
    "\n",
    "def get_s3_keys(bucket, s3_client, prefix = '', foldersOnly=True):\n",
    "    \"\"\"\n",
    "    Generate the keys in an S3 bucket.\n",
    "n fo\n",
    "    :param bucket: Name of the S3 bucket.\n",
    "    :param prefix: Only fetch keys that start with this prefix (optional).\n",
    "    \"\"\"\n",
    "    kwargs = {'Bucket': bucket}\n",
    "    if isinstance(prefix, str):\n",
    "        kwargs['Prefix'] = prefix\n",
    "    if foldersOnly:\n",
    "        kwargs['Delimiter'] ='/'\n",
    "    while True:\n",
    "        resp = s3_client.list_objects_v2(**kwargs)\n",
    "        print(prefix)\n",
    "        print(resp)\n",
    "        if foldersOnly:\n",
    "            for obj in resp['CommonPrefixes']:\n",
    "                    key = obj['Prefix']\n",
    "                    if key.startswith(prefix):\n",
    "                        yield key\n",
    "        else:\n",
    "            for obj in resp['Contents']:\n",
    "                key = obj['Key']\n",
    "                if key.startswith(prefix):\n",
    "                    yield key\n",
    "        try:\n",
    "            kwargs['ContinuationToken'] = resp['NextContinuationToken']\n",
    "        except KeyError:\n",
    "            break\n",
    "\n",
    "def get_folders(folder_path):\n",
    "    keys = get_s3_keys(bucket_name,s3_client,prefix=folder_path)\n",
    "    return list(keys)\n",
    "\n",
    "def get_files(folder_path):\n",
    "    keys = get_s3_keys(bucket_name,s3_client,prefix=folder_path, foldersOnly=False)\n",
    "    return list(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "22d17d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gefs.20240601/00/', 'gefs.20240601/06/', 'gefs.20240601/12/', 'gefs.20240601/18/']\n",
      "['gefs.20240601/00/atmos/', 'gefs.20240601/00/chem/', 'gefs.20240601/00/wave/']\n"
     ]
    }
   ],
   "source": [
    "folders = get_folders('gefs.202406')\n",
    "\n",
    "for init_date in folders:\n",
    "    init_times = get_folders(init_date)\n",
    "    print(prods)\n",
    "    for init_time in init_times:\n",
    "        prod = get_folders(init_time)\n",
    "        print(prod)\n",
    "        for prod in prods:\n",
    "            if 'atmos' in prod:\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d34c8634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gefs.20240601/', 'gefs.20240602/', 'gefs.20240603/', 'gefs.20240604/', 'gefs.20240605/', 'gefs.20240606/', 'gefs.20240607/', 'gefs.20240608/', 'gefs.20240609/', 'gefs.20240610/', 'gefs.20240611/', 'gefs.20240612/', 'gefs.20240613/']\n"
     ]
    }
   ],
   "source": [
    "print(folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4cc40b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = get_folders('gefs.202406')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3c33593d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod0 = prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f60e4a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gefs.20240601/00/atmos/\n"
     ]
    }
   ],
   "source": [
    "prods = prod0\n",
    "for prod in prods:\n",
    "    if 'atmos' in prod:\n",
    "        print(prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "858c6f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gefs.202406**\n",
      "{'ResponseMetadata': {'RequestId': '7YRG6XN6VZE9M0VA', 'HostId': '+4xEUPLzxCVw+sPX25LGYSHpFVzoO+9REw2x3RjITJVDbMlkxHt8DFsT50zpHQqUo8CvF44q8hU=', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amz-id-2': '+4xEUPLzxCVw+sPX25LGYSHpFVzoO+9REw2x3RjITJVDbMlkxHt8DFsT50zpHQqUo8CvF44q8hU=', 'x-amz-request-id': '7YRG6XN6VZE9M0VA', 'date': 'Thu, 13 Jun 2024 17:35:18 GMT', 'x-amz-bucket-region': 'us-east-1', 'content-type': 'application/xml', 'transfer-encoding': 'chunked', 'server': 'AmazonS3'}, 'RetryAttempts': 0}, 'IsTruncated': False, 'Name': 'noaa-gefs-pds', 'Prefix': 'gefs.202406**', 'Delimiter': '/', 'MaxKeys': 1000, 'EncodingType': 'url', 'KeyCount': 0}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'CommonPrefixes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m folders \u001b[38;5;241m=\u001b[39m \u001b[43mget_folders\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgefs.202406**\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[80], line 44\u001b[0m, in \u001b[0;36mget_folders\u001b[0;34m(folder_path)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_folders\u001b[39m(folder_path):\n\u001b[1;32m     43\u001b[0m     keys \u001b[38;5;241m=\u001b[39m get_s3_keys(bucket_name,s3_client,prefix\u001b[38;5;241m=\u001b[39mfolder_path)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[80], line 28\u001b[0m, in \u001b[0;36mget_s3_keys\u001b[0;34m(bucket, s3_client, prefix, foldersOnly)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(resp)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m foldersOnly:\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m \u001b[43mresp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCommonPrefixes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[1;32m     29\u001b[0m             key \u001b[38;5;241m=\u001b[39m obj[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrefix\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     30\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(prefix):\n",
      "\u001b[0;31mKeyError\u001b[0m: 'CommonPrefixes'"
     ]
    }
   ],
   "source": [
    "folders = get_folders('gefs.202406**')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "1a97d702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# this code block from nickorka@github: https://github.com/aws/aws-cli/issues/3784#issuecomment-530432844\n",
    "# regex starts after prefix\n",
    "def search_s3_regex(results, bucket, s3_client, prefix, regex_path):\n",
    "    wc_parts = regex_path.split('/')\n",
    "    if len(wc_parts) == 1 and len(wc_parts[0]) == 0:\n",
    "        results.append(prefix)\n",
    "        return\n",
    "    else:\n",
    "        regex = re.compile(wc_parts[0])\n",
    "        next_regex_path = '/'.join(wc_parts[1:])\n",
    "        paginator = s3_client.get_paginator('list_objects')\n",
    "        \n",
    "        result = paginator.paginate(Bucket=bucket, Delimiter='/', Prefix=prefix)\n",
    "        for pref in result.search('CommonPrefixes'):\n",
    "            if pref is None:\n",
    "                # check files\n",
    "                for k in result.search('Contents'):\n",
    "                    res = k.get('Key')\n",
    "                    search_prefix = res if len(prefix) == 0 else res.split(prefix)[1]\n",
    "                    if re.match(regex, search_prefix):\n",
    "                        results.append(res)\n",
    "            else:\n",
    "                # check paths\n",
    "                res = pref.get('Prefix')\n",
    "                search_prefix = res if len(prefix) == 0 else res.split(prefix)[1]\n",
    "                if re.match(regex, search_prefix):\n",
    "                    search_s3_regex(results, bucket, s3_client, res, next_regex_path)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "43b154a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# HARDCODED REGEX\n",
    "\n",
    "\n",
    "\n",
    "res =[]\n",
    "# regex starts after prefix\n",
    "# get gefs bc 0.5 deg in june\n",
    "\n",
    "search_s3_regex(res, bucket_name, s3_client, 'gefs.202406', '[0-9]{2}/[0-9]{2}/atmos/pgrb2ap5/geavg.t[0-9]{2}z.pgrb2a.0p50.f[0-9].*')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "c60439be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "idxlist = []\n",
    "griblist = []\n",
    "\n",
    "for f in res:\n",
    "    r = re.search(r'.*/geavg.t[0-9]{2}z.pgrb2a.0p50.f(?P<time_step>[0-9][\\d]{2})(?P<ext>\\..*)?', f)\n",
    "\n",
    "    if r:\n",
    "        num_groups = len(r.groups())\n",
    "        if num_groups > 0:\n",
    "            isIDX = False\n",
    "            if num_groups == 2 and r['ext'] == '.idx':\n",
    "                isIDX = True\n",
    "\n",
    "            if int(r['time_step']) <= max_hour:\n",
    "                # valid idx or grib file\n",
    "                #https://noaa-gefs-pds.s3.amazonaws.com/gefs.20240613/00/atmos/pgrb2ap5/geavg.t00z.pgrb2a.0p50.f000\n",
    "                if isIDX:\n",
    "                    idxlist.append(f)\n",
    "                else:\n",
    "                    griblist.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "77885f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_files_by_model_times = dict()\n",
    "grib_files_by_model_times = dict()\n",
    "for idx in idxlist:\n",
    "    r = re.search(r'gefs.(?P<model_date>[0-9]{8})/(?P<model_hour>[0-9]{2})/atmos/pgrb2ap5/geavg.t[0-9]{2}z.pgrb2a.0p50.f(?P<time_step>[0-9][\\d]{2})(?P<ext>\\..*)?', idx)\n",
    "    num_groups = len(r.groups())\n",
    "    if num_groups == 4:\n",
    "        model_time = f\"{r['model_date']}_{r['model_hour']}\"\n",
    "        if model_time not in idx_files_by_model_times:\n",
    "            idx_files_by_model_times[model_time] = []\n",
    "        idx_files_by_model_times[model_time].append(idx)\n",
    "    #url_folder = f'{model_url_folder}/gefs.{model_date}/{model_hour}/atmos/pgrb2ap5/'\n",
    "    \n",
    "for grib in griblist:\n",
    "    r = re.search(r'gefs.(?P<model_date>[0-9]{8})/(?P<model_hour>[0-9]{2})/atmos/pgrb2ap5/geavg.t[0-9]{2}z.pgrb2a.0p50.f(?P<time_step>[0-9][\\d]{2})(?P<ext>\\..*)?', grib)\n",
    "    num_groups = len(r.groups())\n",
    "    if num_groups == 4:\n",
    "        model_time = f\"{r['model_date']}_{r['model_hour']}\"\n",
    "        if model_time not in grib_files_by_model_times:\n",
    "            grib_files_by_model_times[model_time] = []\n",
    "        grib_files_by_model_times[model_time].append(grib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "24cd0740",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_idx_files_by_model_times = dict()\n",
    "num_grib_files_by_model_times = dict()\n",
    "for model_time, idxfiles in idx_files_by_model_times.items():\n",
    "    num_idx_files_by_model_times[model_time] = len(idxfiles)\n",
    "for model_time, gribfiles in grib_files_by_model_times.items():\n",
    "    num_grib_files_by_model_times[model_time] = len(gribfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f46476",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_idx_files_by_model_times = sorted(num_idx_files_by_model_times)\n",
    "num_grib_files_by_model_times = sorted(num_grib_files_by_model_times)\n",
    "if num_idx_files_by_model_times == num_grib_files_by_model_times:\n",
    "    print(\"There are an equal number of idx/grib files\")\n",
    "# sanity check to make sure there are no incomplete directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "40833c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cribbed from download.py / auto_download.py\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import re\n",
    "import requests\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "print_level_importance = 3\n",
    "\n",
    "def print_level(level, warning_str):\n",
    "    if level <= print_level_importance:\n",
    "        print(warning_str)\n",
    "\n",
    "def find_matching_lines_gfs(idx_file_path, params):\n",
    "    matching_lines = []\n",
    "    all_lines = []\n",
    "    with open(idx_file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        for i, line in enumerate(lines):\n",
    "            if any(param in line for param in params):\n",
    "                matching_lines.append((i, line))\n",
    "            if ':' in line:\n",
    "                all_lines.append((i, line))\n",
    "    return all_lines, matching_lines\n",
    "\n",
    "def generate_curl_commands_gfs(timestamp_prefix, idx_file_name, url_folder, url_base_file_name, output_dir, params):\n",
    "    output_dir_timestamp = os.path.join(output_dir, f'gefs.{timestamp_prefix}')\n",
    "    idx_file_path = os.path.join(output_dir, f'gefs.{timestamp_prefix}', f'{timestamp_prefix}_{idx_file_name}')\n",
    "    #os.makedirs(output_dir_timestamp, exist_ok=True)\n",
    "    if not os.path.exists(idx_file_path):\n",
    "        url_idx = f'{url_folder}{idx_file_name}'\n",
    "        curl_command = f\"curl --create-dirs -y 30 -Y 30 -f -v -s {url_idx} -o {idx_file_path}\"\n",
    "        print_level(4, curl_command)\n",
    "\n",
    "        # Run the subprocess and capture its output\n",
    "        result = subprocess.run(curl_command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        time.sleep(sleep_time)\n",
    "        if result.returncode != 0:\n",
    "            print_level(3, f\"Warning: Command failed with exit code {result.returncode}\")\n",
    "            print_level(3, f\"Command: {curl_command}\")\n",
    "            print_level(3, f\"Error output: {result.stderr.decode('utf-8')}\")\n",
    "            try:\n",
    "                # clear directory if no files ready yet\n",
    "                os.rmdir(output_dir_timestamp)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            return -1\n",
    "\n",
    "    all_lines, matching_lines = find_matching_lines_gfs(idx_file_path, params)\n",
    "    total_lines = len(all_lines)\n",
    "\n",
    "    for i, (line_num, line) in enumerate(matching_lines):\n",
    "        fields = line.split(':')\n",
    "        start = int(fields[1])\n",
    "        if i == total_lines - 1:\n",
    "            range_str = f\"{start}-\"\n",
    "            # TODO, right now always overwrite last file? no, change from getting the http size of the url first and setting that as end-1\n",
    "            size = -1\n",
    "        else:\n",
    "            next_line_fields = all_lines[line_num + 1][1].split(':')\n",
    "            end = int(next_line_fields[1]) - 1\n",
    "            range_str = f\"{start}-{end}\"\n",
    "            size = end - start + 1\n",
    "\n",
    "        param_name = fields[3].replace(':', '_').replace(' ', '_').lower()\n",
    "        param_level = fields[4].replace(':', '_').replace(' ', '_').lower()\n",
    "        output_file = os.path.join(output_dir_timestamp, f\"{timestamp_prefix}_{os.path.basename(url_base_file_name)}_{param_name}_{param_level}.grib2\")\n",
    "\n",
    "        overwrite_or_download = True\n",
    "\n",
    "        if os.path.exists(output_file):\n",
    "            if os.path.getsize(output_file) == size:\n",
    "                overwrite_or_download = False\n",
    "\n",
    "        if overwrite_or_download:\n",
    "            url = f'{url_folder}{url_base_file_name}'\n",
    "            curl_command = f\"curl --create-dirs -y 30 -Y 30 -f -v -s -r {range_str} {url} -o {output_file}\"\n",
    "            print_level(4, curl_command)\n",
    "\n",
    "            # Run the subprocess and capture its output\n",
    "            result = subprocess.run(curl_command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            time.sleep(sleep_time)\n",
    "            if result.returncode != 0:\n",
    "                print_level(3, f\"Warning: Command failed with exit code {result.returncode}\")\n",
    "                print_level(3, f\"Command: {curl_command}\")\n",
    "                print_level(3, f\"Error output: {result.stderr.decode('utf-8')}\")\n",
    "                return -2\n",
    "\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "64ce49a1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for model_time, gribfiles in grib_files_by_model_times.items():\n",
    "    timestamp_prefix = model_time\n",
    "    for gribfile in gribfiles:\n",
    "        pathsplit = gribfile.split('/')\n",
    "        url_folder_subdir = \"/\".join(pathsplit[0:-1])\n",
    "        url_folder = f'{model_url_folder}{url_folder_subdir}/'\n",
    "        url_base_file_name = \"/\".join(pathsplit[-1:])\n",
    "        idx_file_name = f\"{url_base_file_name}.idx\"\n",
    "        output_dir = model_data_folder\n",
    "        r = generate_curl_commands_gfs(timestamp_prefix, idx_file_name, url_folder, url_base_file_name, output_dir, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d954aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check in bash shell the smallest grib file:\n",
    "# ls -alS * | grep grib2 | grep 0 | sort -r | tail\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753dfdbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
